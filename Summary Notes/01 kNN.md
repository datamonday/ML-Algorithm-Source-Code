# k-Nearest Neighbors

K近邻算法（k-Nearest Neighbors，k-NN）是一种监督学习算法，可以用于多分类任务和回归任务。其特点是没有训练过程，只有在测试时才需要运算。其核心思想是：给定测试样本和最近邻个数k，根据某一距离度量（一般为欧式距离）计算该样本与训练集中各样本的距离，然后对距离进行排序，取与样本点距离最近的k个样本，根据这k个样本进行预测，如果是分类问题，则使用投票法，取最多的类别作为测试样本的类别；如果是回归问题，则取均值作为预测结果。

k-近邻算法步骤如下：
1. 计算已知类别数据集中的点与当前点(测试样本)之间的距离；
2. 按照距离的递增次序排序；
3. 选取与当前点距离最小的k个点；
4. 确定前k个点所在类别的出现频率；
5. 返回前k个点所出现频率最高的类别作为当前点的预测分类。

## 欧氏距离

$$d(p,q) = d(q,p) = \sqrt{(q_1-p_1)^2 + (q_2-p_2)^2 + ... +(q_n-p_n)^2} = \sqrt{\sum^n_{i=1}(1_i-p_i)^2}$$

---

## 优点

- 简单好用，容易理解，精度高，理论成熟，既可以用来做分类也可以用来做回归；
- 可用于数值型数据和离散型数据；
- 无输入数据假设，仅仅存储训练数据，无明显的训练过程，训练时间复杂度为O(n)；

---
## 缺点

- 因为要算训练样本与所有训练样本的距离，并且需要存储，所以计算复杂性度和空间复杂度高；
- 对k的设置和异常值敏感，比如3近邻算法，距离前三近的样本中一旦有两个样本出现错误值，就足以使预测结果错误，及时在距离稍远处有大量正确的样本；
- 样本不平衡问题（即有些类别的样本数量很多，而其它样本的数量很少）；
- 一般数值很大的时候不用这个，计算量太大。但是单个样本又不能太少，否则容易发生误分。
- 最大的缺点是无法给出数据的内在含义，很多场景下不具备可解释性；
- 维数灾难问题。


---
> ref:https://cuijiahua.com/blog/2017/11/ml_1_knn.html