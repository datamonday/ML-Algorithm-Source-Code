{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,2,3,4,5,6]\n",
    "a.index(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.index(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 言论过滤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob_1: -0.6931471805599453\n",
      "prob_0: -0.500839488252253\n",
      "['love', 'my', 'dalmation'] 属于非侮辱类\n",
      "prob_1: -0.5026709900837548\n",
      "prob_0: -0.6931471805599453\n",
      "['stupid', 'garbage'] 属于侮辱类\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from functools import reduce\n",
    "import re\n",
    "import random\n",
    "random.seed(2020)\n",
    "\n",
    "\n",
    "def load_dataset():\n",
    "    \"\"\"\n",
    "    加载评论数据集，假设数据集已经按照单词切分好\n",
    "    :return: 返回数据集和标签\n",
    "    \"\"\"\n",
    "    # 切分的样本\n",
    "    post_list = [['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "                 ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                 ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "                 ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                 ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "                 ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "\n",
    "    # 类别标签向量，1代表侮辱类 ，0代表非侮辱类\n",
    "    class_vector = [0, 1, 0, 1, 0, 1]\n",
    "\n",
    "    return post_list, class_vector\n",
    "\n",
    "\n",
    "def create_vocab_list(dataset):\n",
    "    \"\"\"\n",
    "    将切分的样本整理成不重复的词汇表(词向量)\n",
    "    :param dataset: 切分的实验样本\n",
    "    :return: 词汇表\n",
    "    \"\"\"\n",
    "    # 创建一个空的不重复的列表\n",
    "    vocab_set = set([])\n",
    "    for doc in dataset:\n",
    "        # 取并集\n",
    "        vocab_set = vocab_set | set(doc)\n",
    "    return list(vocab_set)\n",
    "    # return np.array(list(vocab_set))\n",
    "\n",
    "\n",
    "def set_word2vec(vocab_list, input_data):\n",
    "    \"\"\"\n",
    "    根据vocab_list词汇表，将input_data向量化，向量的每个元素为1或0\n",
    "    :type vocab_list: list\n",
    "    :param vocab_list: createVocabList返回的列表\n",
    "    :param input_data: 切分的词条列表\n",
    "    :return: 文档向量 (词向量)\n",
    "    \"\"\"\n",
    "    # 初始化向量为零向量\n",
    "    word_vector = [0] * len(vocab_list)\n",
    "\n",
    "    for word in input_data:\n",
    "        if word in vocab_list:\n",
    "            # 如果输入数据中的词汇在词汇表中，则词汇向量对应的元素置一\n",
    "            word_vector[vocab_list.index(word)] = 1\n",
    "        else:\n",
    "            print(f\"the word {word} is not in VocabularyList!\")\n",
    "\n",
    "    return word_vector\n",
    "\n",
    "\n",
    "def train_naive_bayes(train_matrix, train_y, laplace=True):\n",
    "    \"\"\"\n",
    "    朴素贝叶斯分类器训练\n",
    "    :param train_matrix: 训练样本\n",
    "    :param train_y: 训练样本标签\n",
    "    :param laplace: 拉普拉斯平滑\n",
    "    :return: 返回预测的两类概率向量以及文档中属于侮辱性的概率\n",
    "    \"\"\"\n",
    "    # 计算训练的文档数目\n",
    "    n_docs = len(train_matrix)\n",
    "    # 计算每篇文档的词条数\n",
    "    n_words_per_doc = len(train_matrix[0])\n",
    "    # 文档属于侮辱类的概率\n",
    "    prob_abusive = sum(train_y) / float(n_docs)\n",
    "    # 创建数组，用于存储单词属于0和1类的概率，np.zeros初始化为0】\n",
    "    prob_0 = np.zeros(n_words_per_doc)\n",
    "    prob_1 = np.zeros(n_words_per_doc)\n",
    "    # 分母初始化为 0.0\n",
    "    prob_0_denominator = 0.0\n",
    "    prob_1_denominator = 0.0\n",
    "\n",
    "    if laplace:\n",
    "        # 分母初始化为 2.0 (二分类)\n",
    "        prob_0_denominator = 2.0\n",
    "        prob_1_denominator = 2.0\n",
    "\n",
    "    for i in range(n_docs):\n",
    "        # 统计属于侮辱类的条件概率所需的数据，即 P(w0|1),P(w1|1),P(w2|1)···\n",
    "        if train_y[i] == 1:\n",
    "            prob_1 += train_matrix[i]\n",
    "            prob_1_denominator += sum(train_matrix[i])\n",
    "        # 统计属于非侮辱类的条件概率所需的数据，即P(w0|0),P(w1|0),P(w2|0)···\n",
    "        else:\n",
    "            prob_0 += train_matrix[i]\n",
    "            prob_0_denominator += sum(train_matrix[i])\n",
    "    # 词向量中，单词属于1类（非侮辱性）的概率向量\n",
    "    prob_1_vector = prob_1 / prob_1_denominator\n",
    "    # 词向量中，单词属于0类的概率向量\n",
    "    prob_0_vector = prob_0 / prob_0_denominator\n",
    "\n",
    "    # 返回属于侮辱类的条件概率数组，属于非侮辱类的条件概率数组，文档属于侮辱类的概率\n",
    "    return prob_0_vector, prob_1_vector, prob_abusive\n",
    "\n",
    "\n",
    "def navie_bayes_classifer(input_vector, prob_0_vector, prob_1_vector, prob_abusive, log=True):\n",
    "    \"\"\"\n",
    "    贝叶斯分类器\n",
    "    :param input_vector: 待分类的词向量\n",
    "    :param prob_0_vector: 属于0类的概率向量\n",
    "    :param prob_1_vector: 属于1类的概率向量\n",
    "    :param prob_abusive: 词向量属于1类的概率\n",
    "    :param log: 防止造成下溢\n",
    "    :return: 0或1\n",
    "    \"\"\"\n",
    "    # reduce() 函数会对参数序列中元素进行累积。\n",
    "    prob_1 = reduce(lambda x, y : x * y, input_vector * prob_1_vector) * prob_abusive\n",
    "    prob_0 = reduce(lambda x, y : x * y, input_vector * prob_0_vector) * prob_abusive\n",
    "\n",
    "    if log:\n",
    "        # 对应元素相乘 logA * B = logA + logB，所以这里加上log(pClass1)\n",
    "        prob_1 = sum(input_vector * prob_1_vector) + np.log(prob_abusive)\n",
    "        prob_0 = sum(input_vector * prob_0_vector) + np.log(1.0 - prob_abusive)\n",
    "\n",
    "    print(\"prob_1:\", prob_1)\n",
    "    print(\"prob_0:\", prob_0)\n",
    "    if prob_1 > prob_0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def test_nave_bayes(test_vocab):\n",
    "    post_list, class_vector = load_dataset()\n",
    "    vocab_list = create_vocab_list(post_list)\n",
    "    train_matrix = []\n",
    "    for post_in_doc in post_list:\n",
    "        train_matrix.append((set_word2vec(vocab_list, post_in_doc)))\n",
    "    prob_0_vector, prob_1_vector, prob_abusive = train_naive_bayes(train_matrix, class_vector)\n",
    "\n",
    "    test_vector = np.array(set_word2vec(vocab_list, test_vocab))\n",
    "    if navie_bayes_classifer(test_vector, prob_0_vector, prob_1_vector, prob_abusive):\n",
    "        print(test_vocab, '属于侮辱类')  # 执行分类并打印分类结果\n",
    "    else:\n",
    "        print(test_vocab, '属于非侮辱类')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # post_list, class_vector = load_dataset()\n",
    "    # print(\"post_list:\\n\", post_list)\n",
    "    #\n",
    "    # vocab_list = create_vocab_list(post_list)\n",
    "    # print(\"vocab_list:\\n\", vocab_list)\n",
    "    # print(\"vocab_list.shape:\", len(vocab_list))\n",
    "    #\n",
    "    # train_matrix = []\n",
    "    # for post_in_doc in post_list:\n",
    "    #     train_matrix.append((set_word2vec(vocab_list, post_in_doc)))\n",
    "    # print(\"train_matrix:\\n\", train_matrix)\n",
    "    # print(\"train_matrix.shape:\", np.array(train_matrix).shape)\n",
    "    #\n",
    "    # # --------------------- train Naive Bayes Classifier ---------------------\n",
    "    # prob_0_vector, prob_1_vector, prob_abusive = train_naive_bayes(train_matrix, class_vector)\n",
    "    # print(\"prob_0_vector:\\n\", prob_0_vector)\n",
    "    # print(\"prob_1_vector:\\n\", prob_1_vector)\n",
    "    #\n",
    "    # print(\"class_vector:\", class_vector)\n",
    "    # # prob_abusive是所有侮辱类的样本占所有样本的概率，从class_vector中可以看出，一用有3个侮辱类，3个非侮辱类。所以侮辱类的概率是0.5\n",
    "    # print(\"prob_abusive:\", prob_abusive)\n",
    "\n",
    "    # ------------------- Naive Bayes Classifier predict ---------------------\n",
    "    # 会发现，算法无法进行分类，p0和p1的计算结果都是0，显然结果错误，需要进行改进——拉普拉斯平滑(Laplace Smoothing)！\n",
    "    # 另外一个遇到的问题就是下溢出，这是由于太多很小的数相乘造成的。通过求对数可以避免下溢出或者浮点数舍入导致的错误。\n",
    "    test_vocab1 = ['love', 'my', 'dalmation']\n",
    "    test_nave_bayes(test_vocab1)\n",
    "\n",
    "    test_vocab2 = ['stupid', 'garbage']\n",
    "    test_nave_bayes(test_vocab2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 垃圾邮件分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob_1: -0.5098047673474788\n",
      "prob_0: -0.5465947647307757\n",
      "prob_1: -0.7294700393140059\n",
      "prob_0: -0.5792034603829496\n",
      "分类错误测试集： ['codeine', '15mg', 'for', '203', 'visa', 'only', 'codeine', 'methylmorphine', 'narcotic', 'opioid', 'pain', 'reliever', 'have', '15mg', '30mg', 'pills', '15mg', 'for', '203', '15mg', 'for', '385', '15mg', 'for', '562', 'visa', 'only']\n",
      "prob_1: -0.547461671113169\n",
      "prob_0: -0.5170916591407135\n",
      "分类错误测试集： ['get', 'off', 'online', 'watchesstore', 'discount', 'watches', 'for', 'all', 'famous', 'brands', 'watches', 'arolexbvlgari', 'dior', 'hermes', 'oris', 'cartier', 'and', 'more', 'brands', 'louis', 'vuitton', 'bags', 'wallets', 'gucci', 'bags', 'tiffany', 'jewerly', 'enjoy', 'full', 'year', 'warranty', 'shipment', 'via', 'reputable', 'courier', 'fedex', 'ups', 'dhl', 'and', 'ems', 'speedpost', 'you', 'will', '100', 'recieve', 'your', 'order']\n",
      "prob_1: -0.7880474451717465\n",
      "prob_0: -0.5636755100723906\n",
      "prob_1: -0.7608507924520812\n",
      "prob_0: -0.5978370007556204\n",
      "分类错误测试集： ['buy', 'ambiem', 'zolpidem', '5mg', '10mg', 'pill', 'pills', '129', 'pills', '199', '180', 'pills', '430', 'pills', '138', '120', 'pills', '322']\n",
      "prob_1: -0.5098047673474788\n",
      "prob_0: -0.5465947647307757\n",
      "prob_1: -0.7817712945441314\n",
      "prob_0: -0.5326196094512726\n",
      "prob_1: -0.6959972359667256\n",
      "prob_0: -0.4083960069668005\n",
      "prob_1: -0.4909763154646336\n",
      "prob_0: -0.5170916591407135\n",
      "prob_1: -0.7608507924520812\n",
      "prob_0: -0.5652283051034465\n",
      "分类错误测试集： ['oem', 'adobe', 'microsoft', 'softwares', 'fast', 'order', 'and', 'download', 'microsoft', 'office', 'professional', 'plus', '2007', '2010', '129', 'microsoft', 'windows', 'ultimate', '119', 'adobe', 'photoshop', 'cs5', 'extended', 'adobe', 'acrobat', 'pro', 'extended', 'windows', 'professional', 'thousand', 'more', 'titles']\n",
      "错误率：40.00%\n"
     ]
    }
   ],
   "source": [
    "def bag_word2vec(vocab_list, input_set):\n",
    "    \"\"\"\n",
    "    根据 vocab_list词汇表，构建词袋模型\n",
    "    :param vocab_list: creat_vocab_list 返回的词汇表（列表）\n",
    "    :param input_set: 切分的词条列表\n",
    "    :return: 文档向量（词袋模型）\n",
    "    \"\"\"\n",
    "    vocab_vector = [0] * len(vocab_list)\n",
    "    for word in input_set:\n",
    "        if word in vocab_list:\n",
    "            vocab_vector[vocab_list.index(word)] += 1\n",
    "\n",
    "    return vocab_vector\n",
    "\n",
    "\n",
    "def str_to_list(text):\n",
    "    \"\"\"\n",
    "    接收一个大字符串并将其解析为字符串列表\n",
    "    :param text: 大字符串\n",
    "    :return: 字符串列表\n",
    "    \"\"\"\n",
    "    # 将特殊符号作为切分标志进行字符串切分，即非字母、非数字\n",
    "    list_of_tokens = re.split(r'\\W+', text)\n",
    "\n",
    "    return [token.lower() for token in list_of_tokens if len(token) > 2]\n",
    "\n",
    "\n",
    "def spam_classifier(sklearn=True):\n",
    "    \"\"\"\n",
    "    垃圾邮件分类\n",
    "    ham：废垃圾邮件；spam：垃圾邮件\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    rootdir = 'D:/Github/ML-Algorithm-Source-Code/'\n",
    "    spam_filepath = rootdir + 'dataset/email/spam/'\n",
    "    ham_filepath = rootdir + 'dataset/email/ham/'\n",
    "\n",
    "    doc_list = []\n",
    "    class_list = []\n",
    "    full_text = []\n",
    "\n",
    "    # 遍历 25个 txt 文件\n",
    "    for i in range(1, 26):\n",
    "        # 读取每个垃圾邮件，并字符串转换成字符串列表\n",
    "        word_list = str_to_list(open(spam_filepath + '%d.txt' % i, 'r').read())\n",
    "        doc_list.append(word_list)\n",
    "        full_text.append(word_list)\n",
    "        class_list.append(1)\n",
    "        word_list = str_to_list(open(ham_filepath + '%d.txt' % i, 'r').read())\n",
    "        doc_list.append(word_list)\n",
    "        full_text.append(word_list)\n",
    "        class_list.append(0)\n",
    "\n",
    "    # 创建词汇表，不重复\n",
    "    vocab_list = create_vocab_list(doc_list)\n",
    "    dataset = list(range(50))\n",
    "    test_x = []\n",
    "    # 从50个邮件中，随机挑选出40个作为训练集，10个做测试集\n",
    "    # 随机选取10个，构造测试集\n",
    "    for i in range(10):\n",
    "        rand_index = int(random.uniform(0, len(dataset)))\n",
    "        test_x.append(dataset[rand_index])\n",
    "        del(dataset[rand_index])\n",
    "    train_x = []\n",
    "    train_y = []\n",
    "    # 遍历训练集\n",
    "    for doc_index in dataset:\n",
    "        # 将生成的词袋模型添加到训练矩阵中\n",
    "        train_x.append(set_word2vec(vocab_list, doc_list[doc_index]))\n",
    "        # 将类别添加到训练集类别标签向量中\n",
    "        train_y.append(class_list[doc_index])\n",
    "    # 训练朴素贝叶斯模型\n",
    "    prob_0_vector, prob_1_vector, prob_spam = train_naive_bayes(np.array(train_x), np.array(train_y))\n",
    "    # 错误分类计数\n",
    "    error_count = 0\n",
    "    # 遍历测试集\n",
    "    for doc_index in test_x:\n",
    "        word_vector = set_word2vec(vocab_list, doc_list[doc_index])\n",
    "        if navie_bayes_classifer(np.array(word_vector), prob_0_vector, prob_1_vector, prob_spam) != class_list[doc_index]:\n",
    "            error_count += 1\n",
    "            print(\"分类错误测试集：\", doc_list[doc_index])\n",
    "\n",
    "    print(\"错误率：%.2f%%\" % (float(error_count) / len(test_x) * 100))\n",
    "    \n",
    "    if sklearn:\n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    spam_classifier()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn 新闻分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob_1: -0.7046790435359651\n",
      "prob_0: -0.6155108625443594\n",
      "prob_1: -0.6887744709713527\n",
      "prob_0: -0.5594211189546158\n",
      "error test set： ['benoit', 'mandelbrot', '1924', '2010', 'benoit', 'mandelbrot', '1924', '2010', 'wilmott', 'team', 'benoit', 'mandelbrot', 'the', 'mathematician', 'the', 'father', 'fractal', 'mathematics', 'and', 'advocate', 'more', 'sophisticated', 'modelling', 'quantitative', 'finance', 'died', '14th', 'october', '2010', 'aged', 'wilmott', 'magazine', 'has', 'often', 'featured', 'mandelbrot', 'his', 'ideas', 'and', 'the', 'work', 'others', 'inspired', 'his', 'fundamental', 'insights', 'you', 'must', 'logged', 'view', 'these', 'articles', 'from', 'past', 'issues', 'wilmott', 'magazine']\n",
      "prob_1: -0.46412238349620377\n",
      "prob_0: -0.5978826574161542\n",
      "error test set： ['you', 'have', 'everything', 'gain', 'incredib1e', 'gains', 'length', 'inches', 'yourpenis', 'permanantly', 'amazing', 'increase', 'thickness', 'yourpenis', 'betterejacu1ation', 'control', 'experience', 'rock', 'harderecetions', 'explosive', 'intenseorgasns', 'increase', 'volume', 'ofejacu1ate', 'doctor', 'designed', 'and', 'endorsed', '100', 'herbal', '100', 'natural', '100', 'safe']\n",
      "prob_1: -0.6947386856830824\n",
      "prob_0: -0.5514082984417953\n",
      "error test set： ['thanks', 'peter', 'definitely', 'check', 'this', 'how', 'your', 'book', 'going', 'heard', 'chapter', 'came', 'and', 'was', 'good', 'shape', 'hope', 'you', 'are', 'doing', 'well', 'cheers', 'troy']\n",
      "prob_1: -0.44622973936101484\n",
      "prob_0: -0.562626247159744\n",
      "error test set： ['you', 'have', 'everything', 'gain', 'incredib1e', 'gains', 'length', 'inches', 'yourpenis', 'permanantly', 'amazing', 'increase', 'thickness', 'yourpenis', 'betterejacu1ation', 'control', 'experience', 'rock', 'harderecetions', 'explosive', 'intenseorgasns', 'increase', 'volume', 'ofejacu1ate', 'doctor', 'designed', 'and', 'endorsed', '100', 'herbal', '100', 'natural', '100', 'safe', 'the', 'proven', 'naturalpenisenhancement', 'that', 'works', '100', 'moneyback', 'guaranteeed']\n",
      "prob_1: -0.6808221846890465\n",
      "prob_0: -0.5273698369033337\n",
      "error test set： ['peter', 'sure', 'thing', 'sounds', 'good', 'let', 'know', 'what', 'time', 'would', 'good', 'for', 'you', 'will', 'come', 'prepared', 'with', 'some', 'ideas', 'and', 'can', 'from', 'there', 'regards', 'vivek']\n",
      "prob_1: -0.6172038944305973\n",
      "prob_0: -0.5930749651084619\n",
      "prob_1: -0.46412238349620377\n",
      "prob_0: -0.5978826574161542\n",
      "error test set： ['you', 'have', 'everything', 'gain', 'incredib1e', 'gains', 'length', 'inches', 'yourpenis', 'permanantly', 'amazing', 'increase', 'thickness', 'yourpenis', 'betterejacu1ation', 'control', 'experience', 'rock', 'harderecetions', 'explosive', 'intenseorgasns', 'increase', 'volume', 'ofejacu1ate', 'doctor', 'designed', 'and', 'endorsed', '100', 'herbal', '100', 'natural', '100', 'safe']\n",
      "prob_1: -0.7026909719653885\n",
      "prob_0: -0.6299339394674364\n",
      "prob_1: -0.7166074729594243\n",
      "prob_0: -0.5818570163905132\n",
      "error test set： ['arvind', 'thirumalai', 'commented', 'your', 'status', 'arvind', 'wrote', 'you', 'know', 'reply', 'this', 'email', 'comment', 'this', 'status']\n",
      "--------------------------------\n",
      "Self NB test acc：70.00%\n",
      "sklearn NB test acc:  1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "def bag_word2vec(vocab_list, input_set):\n",
    "    \"\"\"\n",
    "    根据 vocab_list词汇表，构建词袋模型\n",
    "    :param vocab_list: creat_vocab_list 返回的词汇表（列表）\n",
    "    :param input_set: 切分的词条列表\n",
    "    :return: 文档向量（词袋模型）\n",
    "    \"\"\"\n",
    "    vocab_vector = [0] * len(vocab_list)\n",
    "    for word in input_set:\n",
    "        if word in vocab_list:\n",
    "            vocab_vector[vocab_list.index(word)] += 1\n",
    "\n",
    "    return vocab_vector\n",
    "\n",
    "\n",
    "def str_to_list(text):\n",
    "    \"\"\"\n",
    "    接收一个大字符串并将其解析为字符串列表\n",
    "    :param text: 大字符串\n",
    "    :return: 字符串列表\n",
    "    \"\"\"\n",
    "    # 将特殊符号作为切分标志进行字符串切分，即非字母、非数字\n",
    "    list_of_tokens = re.split(r'\\W+', text)\n",
    "\n",
    "    return [token.lower() for token in list_of_tokens if len(token) > 2]\n",
    "\n",
    "\n",
    "def spam_classifier(sklearn=True):\n",
    "    \"\"\"\n",
    "    垃圾邮件分类\n",
    "    ham：废垃圾邮件；spam：垃圾邮件\n",
    "    :param sklearn: 使用sklearn的api进行测试\n",
    "    \"\"\"\n",
    "    rootdir = 'D:/Github/ML-Algorithm-Source-Code/'\n",
    "    spam_filepath = rootdir + 'dataset/email/spam/'\n",
    "    ham_filepath = rootdir + 'dataset/email/ham/'\n",
    "\n",
    "    doc_list = []\n",
    "    class_list = []\n",
    "    full_text = []\n",
    "\n",
    "    # 遍历 25个 txt 文件\n",
    "    for i in range(1, 26):\n",
    "        # 读取每个垃圾邮件，并字符串转换成字符串列表\n",
    "        word_list = str_to_list(open(spam_filepath + '%d.txt' % i, 'r').read())\n",
    "        doc_list.append(word_list)\n",
    "        full_text.append(word_list)\n",
    "        class_list.append(1)\n",
    "        word_list = str_to_list(open(ham_filepath + '%d.txt' % i, 'r').read())\n",
    "        doc_list.append(word_list)\n",
    "        full_text.append(word_list)\n",
    "        class_list.append(0)\n",
    "\n",
    "    # 创建词汇表，不重复\n",
    "    vocab_list = create_vocab_list(doc_list)\n",
    "    dataset = list(range(50))\n",
    "    test_x = []\n",
    "\n",
    "    # 从50个邮件中，随机挑选出40个作为训练集，10个做测试集\n",
    "    # 随机选取10个，构造测试集\n",
    "    for i in range(10):\n",
    "        rand_index = int(random.uniform(0, len(dataset)))\n",
    "        test_x.append(dataset[rand_index])\n",
    "        del(dataset[rand_index])\n",
    "\n",
    "    train_x = []\n",
    "    train_y = []\n",
    "\n",
    "    # 遍历训练集\n",
    "    for doc_index in dataset:\n",
    "        # 将生成的词袋模型添加到训练矩阵中\n",
    "        train_x.append(set_word2vec(vocab_list, doc_list[doc_index]))\n",
    "        # 将类别添加到训练集类别标签向量中\n",
    "        train_y.append(class_list[doc_index])\n",
    "    # 训练朴素贝叶斯模型\n",
    "    prob_0_vector, prob_1_vector, prob_spam = train_naive_bayes(np.array(train_x), np.array(train_y))\n",
    "\n",
    "   # 正确分类计数\n",
    "    true_count = 0\n",
    "\n",
    "    # 遍历测试集\n",
    "    for doc_index in test_x:\n",
    "        word_vector = set_word2vec(vocab_list, doc_list[doc_index])\n",
    "        if navie_bayes_classifer(np.array(word_vector), prob_0_vector, prob_1_vector, prob_spam) == class_list[doc_index]:\n",
    "            true_count += 1\n",
    "            print(\"error test set：\", doc_list[doc_index])\n",
    "\n",
    "    print('-' * 32)\n",
    "    print(\"Self NB test acc：%.2f%%\" % ((float(true_count) / len(test_x)) * 100))\n",
    "\n",
    "    if sklearn:\n",
    "        test_x = []\n",
    "        test_y = []\n",
    "        # 随机选取10个，构造测试集\n",
    "        for i in range(10):\n",
    "            rand_index = int(random.uniform(0, len(dataset)))\n",
    "            test_x.append(set_word2vec(vocab_list, doc_list[rand_index]))\n",
    "            test_y.append(class_list[rand_index])\n",
    "            del (dataset[rand_index])\n",
    "\n",
    "        clf = MultinomialNB()\n",
    "        clf.fit(np.array(train_x), np.array(train_y).ravel())\n",
    "        # clf_pred = clf.predict(np.array(test_x))\n",
    "        test_acc = clf.score(np.array(test_x), np.array(test_y).ravel())\n",
    "        print(\"sklearn NB test acc: \", test_acc)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    spam_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference：\n",
    "> https://cuijiahua.com/blog/2017/11/ml_5_bayes_2.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
